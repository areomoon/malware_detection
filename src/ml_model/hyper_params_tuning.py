
import argparse
import numpy as np
import pandas as pd
import lightgbm as lgb
from bayes_opt import BayesianOptimization

parser = argparse.ArgumentParser(description='data processing')
parser.add_argument('--params_save_name', default="best_p.txt",help='Write a hyperparams file save name')
args = parser.parse_args()


# Load the preprocessed data
# Access data store
data_store = pd.HDFStore('processed_data.h5')

# Retrieve data using key
train = data_store['preprocessed_train']
test = data_store['preprocessed_test']
data_store.close()

features = train.columns.difference(['md5','label'])
d_train = lgb.Dataset(train[features], label=train['label'])

# Objective Function
def hyp_lgbm(num_leaves, max_depth, learning_rate, lambda_l1, lambda_l2):
    params = {
      "objective" : "multiclass",
      "num_class" : 10,
      "bagging_fraction" : 0.8,  # subsample
      "feature_fraction" : 0.8,  # colsample_bytree
      "bagging_freq" : 5,        # subsample_freq
      "bagging_seed" : 2018,
      "num_threads":6,
      "verbosity" : -1
    }

    params["num_leaves"] = int(round(num_leaves))
    params['max_depth'] = int(round(max_depth))
    params["learning_rate"]= round(learning_rate,2)
    params['lambda_l1'] = round(lambda_l1,2)
    params['lambda_l2'] = round(lambda_l2,2)

    cv_results = lgb.cv(params, d_train, nfold=2,stratified=True, seed=17, num_boost_round=300,verbose_eval=50,early_stopping_rounds=100)
    return -np.min(cv_results['multi_logloss-mean'])

# Domain space-- Range of hyperparameters
pds = {'num_leaves': (10, 70),
       'max_depth': (3, 6),
       'learning_rate':(0.03, 0.08),
       'lambda_l1': (0.3,1),
       'lambda_l2': (0.5,1),
       }

# Surrogate model
optimizer = BayesianOptimization(hyp_lgbm, pds,random_state=7)

# Optimize
optimizer.maximize(init_points=3, n_iter=10)
best_params= optimizer.max['params']
print(optimizer.max)
print(best_params)

# Save as txt
f = open(args.params_save_name,"w")
f.write(str(best_params))
f.close()