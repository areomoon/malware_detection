
import pandas as pd
import lightgbm as lgb

# Load the preprocessed data
# Access data store
data_store = pd.HDFStore('processed_data.h5')

# Retrieve data using key
train = data_store['preprocessed_train']
test = data_store['preprocessed_test']
data_store.close()


# 1-gram, 4-gram, byte-related fast features(blocksizedistribution, total lines, etc.), tcnn
params = {
          "objective" : "multiclass",
          "num_class" : 10,
          "num_leaves" : 50,
          "max_depth": 6,
          "learning_rate" : 0.03,
          "bagging_fraction" : 0.8,  # subsample
          "feature_fraction" : 0.8,  # colsample_bytree
          "bagging_freq" : 5,        # subsample_freq
          "bagging_seed" : 2018,
          'lambda_l2': 0.5,
          "lambda_l1": 1.0,
          "num_threads":6,
          "verbosity" : -1 }

features = train.columns.difference(['md5','label'])
d_train = lgb.Dataset(train[features], label=train['label'])

cv_results = lgb.cv(params, d_train, num_boost_round=500, nfold=5,
                    verbose_eval=50, early_stopping_rounds=100)

print('Current parameters:\n', params)
print('\nBest num_boost_round:', len(cv_results['multi_logloss-mean']))
print('Best CV score:', cv_results['multi_logloss-mean'][-1])
print('Current parameters:\n', params)
print('\nBest num_boost_round:', len(cv_results['multi_logloss-mean']))
print('Best CV score:', cv_results['multi_logloss-mean'][-1])